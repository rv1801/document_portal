{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "809448d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f64e049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc36313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9835f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a45f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20371631",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4194c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7983ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##this is an experimental value there is not deteministic value for chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6c75df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6622f298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/home/viren/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}\n",
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗ Louis Martin† Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0b42ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9444fab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bb93efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f64ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2329dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorestore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95ba5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relavant_docs = vectorestore.similarity_search(\"What are the scores of the Llama2 models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b254e11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 models and others open-source models.\\nStandard Benchmarks. In Table 20, we show results on several standard benchmarks.\\nCode Generation. In Table 21, we compare results ofLlama 2with popular open source models on the\\nHuman-Eval and MBPP code generation benchmarks.\\nWorld Knowledge. We evaluate theLlama 2model together with other open-source models on the Natu-\\nralQuestions and TriviaQA benchmarks (Table 22).'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relavant_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d871db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorestore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f82f9e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'keywords': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page_label': '48', 'title': '', 'source': '/home/viren/document_portal/notebook/data/sample.pdf', 'producer': 'pdfTeX-1.40.25', 'total_pages': 77, 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'moddate': '2023-07-20T00:30:36+00:00', 'page': 47, 'subject': '', 'trapped': '/False', 'author': ''}, page_content='2 models and others open-source models.\\nStandard Benchmarks. In Table 20, we show results on several standard benchmarks.\\nCode Generation. In Table 21, we compare results ofLlama 2with popular open source models on the\\nHuman-Eval and MBPP code generation benchmarks.\\nWorld Knowledge. We evaluate theLlama 2model together with other open-source models on the Natu-\\nralQuestions and TriviaQA benchmarks (Table 22).'),\n",
       " Document(metadata={'author': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'trapped': '/False', 'total_pages': 77, 'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': '2023-07-20T00:30:36+00:00', 'creationdate': '2023-07-20T00:30:36+00:00', 'page_label': '74', 'source': '/home/viren/document_portal/notebook/data/sample.pdf', 'subject': '', 'keywords': '', 'page': 73}, page_content='Llama 1\\n7B 0.27 0.26 0.34 0.54 0.36 0.39 0.26 0.28 0.33 0.45 0.33 0.17 0.24 0.31 0.44 0.57 0.39 0.3513B 0.24 0.24 0.31 0.52 0.37 0.37 0.23 0.28 0.31 0.50 0.27 0.10 0.24 0.27 0.41 0.55 0.34 0.2533B 0.23 0.26 0.34 0.50 0.36 0.35 0.24 0.33 0.34 0.49 0.31 0.12 0.23 0.30 0.41 0.60 0.28 0.2765B 0.25 0.26 0.34 0.46 0.36 0.40 0.25 0.32 0.32 0.48 0.31 0.11 0.25 0.30 0.43 0.60 0.39 0.34\\nLlama 2'),\n",
       " Document(metadata={'title': '', 'moddate': '2023-07-20T00:30:36+00:00', 'author': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'producer': 'pdfTeX-1.40.25', 'page': 70, 'creationdate': '2023-07-20T00:30:36+00:00', 'page_label': '71', 'source': '/home/viren/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'keywords': '', 'creator': 'LaTeX with hyperref', 'trapped': '/False'}, page_content='65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\\nLlama 2\\n7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\\n13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\\n34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\\n70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\\nFine-tuned\\nChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66'),\n",
       " Document(metadata={'title': '', 'producer': 'pdfTeX-1.40.25', 'moddate': '2023-07-20T00:30:36+00:00', 'total_pages': 77, 'keywords': '', 'page_label': '8', 'source': '/home/viren/document_portal/notebook/data/sample.pdf', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'page': 7, 'trapped': '/False', 'creator': 'LaTeX with hyperref'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(metadata={'author': '', 'source': '/home/viren/document_portal/notebook/data/sample.pdf', 'title': '', 'subject': '', 'page_label': '74', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'keywords': '', 'creationdate': '2023-07-20T00:30:36+00:00', 'page': 73, 'creator': 'LaTeX with hyperref', 'total_pages': 77, 'moddate': '2023-07-20T00:30:36+00:00', 'trapped': '/False', 'producer': 'pdfTeX-1.40.25'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(metadata={'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'author': '', 'subject': '', 'keywords': '', 'trapped': '/False', 'creationdate': '2023-07-20T00:30:36+00:00', 'creator': 'LaTeX with hyperref', 'source': '/home/viren/document_portal/notebook/data/sample.pdf', 'page': 6, 'producer': 'pdfTeX-1.40.25', 'moddate': '2023-07-20T00:30:36+00:00', 'title': '', 'total_pages': 77, 'page_label': '7'}, page_content='2.3 Llama 2Pretrained Model Evaluation\\nIn this section, we report the results for theLlama 1and Llama 2base models, MosaicML Pretrained\\nTransformer(MPT)†† models,andFalcon(Almazroueietal.,2023)modelsonstandardacademicbenchmarks.\\nFor all the evaluations, we use our internal evaluations library. We reproduce results for the MPT and Falcon\\nmodels internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.'),\n",
       " Document(metadata={'subject': '', 'page_label': '70', 'producer': 'pdfTeX-1.40.25', 'title': '', 'creator': 'LaTeX with hyperref', 'source': '/home/viren/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'keywords': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'author': '', 'page': 69, 'trapped': '/False', 'creationdate': '2023-07-20T00:30:36+00:00', 'moddate': '2023-07-20T00:30:36+00:00'}, page_content='% (true + info) % true % info\\nPretrained\\nMPT 7B 29.13 36.72 92.04\\n30B 35.25 40.27 94.74\\nFalcon 7B 25.95 29.01 96.08\\n40B 40.39 44.80 95.23\\nLlama 1\\n7B 27.42 32.31 94.86\\n13B 41.74 45.78 95.72\\n33B 44.19 48.71 95.23\\n65B 48.71 51.29 96.82\\nLlama 2\\n7B 33.29 39.53 93.02\\n13B 41.86 45.65 96.08\\n34B 43.45 46.14 96.7\\n70B 50.18 53.37 96.21\\nFine-tuned\\nChatGPT 78.46 79.92 98.53\\nMPT-instruct 7B 29.99 35.13 94.37\\nFalcon-instruct 7B 28.03 41.00 85.68\\nLlama 2-Chat\\n7B 57.04 60.59 96.45\\n13B 62.18 65.73 96.45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'author': '', 'source': '/home/viren/document_portal/notebook/data/sample.pdf', 'creationdate': '2023-07-20T00:30:36+00:00', 'subject': '', 'page': 7, 'total_pages': 77, 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': '', 'page_label': '8'}, page_content='Model Size Code Commonsense\\nReasoning\\nWorld\\nKnowledge\\nReading\\nComprehensionMath MMLU BBH AGI Eval\\nMPT 7B 20.5 57.4 41.0 57.5 4.9 26.8 31.0 23.5\\n30B 28.9 64.9 50.0 64.7 9.1 46.9 38.0 33.8\\nFalcon 7B 5.6 56.1 42.8 36.0 4.6 26.2 28.0 21.2\\n40B 15.2 69.2 56.7 65.7 12.6 55.4 37.1 37.0\\nLlama 1\\n7B 14.1 60.8 46.2 58.5 6.95 35.1 30.3 23.9\\n13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2'),\n",
       " Document(metadata={'page': 48, 'author': '', 'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/home/viren/document_portal/notebook/data/sample.pdf', 'creationdate': '2023-07-20T00:30:36+00:00', 'creator': 'LaTeX with hyperref', 'page_label': '49', 'producer': 'pdfTeX-1.40.25', 'moddate': '2023-07-20T00:30:36+00:00', 'keywords': '', 'subject': '', 'trapped': '/False', 'total_pages': 77}, page_content='Llama 2\\n7B 77.4 78.8 48.3 77.2 69.2 75.2 45.9 58.6 57.8 45.3\\n13B 81.7 80.5 50.3 80.7 72.8 77.3 49.4 57.0 67.3 54.8\\n34B 83.7 81.9 50.9 83.3 76.7 79.4 54.5 58.2 74.3 62.6\\n70B 85.0 82.8 50.7 85.3 80.2 80.2 57.4 60.2 78.5 68.9\\nTable 20: Performance on standard benchmarks.\\nHuman-Eval MBPP\\npass@1 pass@100 pass@1 pass@80\\nMPT 7B 18.3 - 22.6 -\\n30B 25.0 - 32.8 -\\nFalcon 7B 0.0 - 11.2 -\\n40B 0.6 - 29.8 -\\nLlama 1\\n7B 10.5 36.5 17.7 56.2\\n13B 15.8 52.5 22.0 64.0\\n33B 21.7 70.7 30.2 73.4\\n65B 23.7 79.3 37.7 76.8'),\n",
       " Document(metadata={'author': '', 'subject': '', 'trapped': '/False', 'page': 4, 'producer': 'pdfTeX-1.40.25', 'keywords': '', 'page_label': '5', 'source': '/home/viren/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'creator': 'LaTeX with hyperref', 'moddate': '2023-07-20T00:30:36+00:00', 'creationdate': '2023-07-20T00:30:36+00:00', 'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'}, page_content='2 Pretraining\\nTocreatethenewfamilyof Llama 2models,webeganwiththepretrainingapproachdescribedinTouvronetal.\\n(2023), using an optimized auto-regressive transformer, but made several changes to improve performance.\\nSpecifically, we performed more robust data cleaning, updated our data mixes, trained on 40% more total\\ntokens,doubledthecontextlength,andusedgrouped-queryattention(GQA)toimproveinferencescalability')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What are the scores of the Llama2 models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7116c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Answer the question based on the context provided below. If the context does not contain the answer, say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3cb41129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f8c5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc703c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context provided below. If the context does not contain the answer, say \"I don\\'t know\".\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\\n')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20b07395",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b224cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d1000cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | Prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3931902d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so I need to figure out the reward model usage in Llama2 models based on the provided context. Let me read through the context carefully to extract the relevant information.\\n\\nFirst, I see that the context mentions the reward model several times. It says the reward model takes a model response and its prompt, including previous contexts, and outputs a scalar score indicating quality aspects like helpfulness and safety. This score is used as a reward to optimize Llama 2-Chat during RLHF (Reinforcement Learning with Human Feedback) to align better with human preferences and improve helpfulness and safety.\\n\\nIt also talks about how the reward models were trained using human annotations collected over time. These annotations were done weekly, and as more data was collected, the reward models improved. There's mention of evaluating model variants with different percentages of safety data using these reward models. \\n\\nThe context notes that the reward model's accuracy can degrade if it's not exposed to new data distributions from updated models. So, they gather new preference data with each new Llama 2-Chat iteration to keep the reward model accurate. There's a figure mentioned (Figure 29) showing the correlation between reward scores and human ratings, which suggests the reward model's effectiveness.\\n\\nAdditionally, the reward models are compared against baselines, including GPT-4, and they outperform them. This indicates the reward models are effective in their intended tasks of assessing helpfulness and safety.\\n\\nPutting this together, the reward model is a crucial component used during the RLHF process to train Llama 2-Chat. It assesses the quality of responses, provides rewards for optimization, and requires continuous updating with new data to maintain accuracy as the model evolves.\\n</think>\\n\\nThe reward model in Llama2 models is integral to the training process, particularly during Reinforcement Learning with Human Feedback (RLHF). It evaluates the quality of model responses based on factors like helpfulness and safety, providing scalar scores that guide optimization. The model is trained using human annotations collected over time, with continuous updates to maintain accuracy as new model iterations are developed. This approach ensures alignment with human preferences and enhances model performance.\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Can you give me reward model usage in Llama2 models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b2e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
